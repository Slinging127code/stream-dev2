[DwsTradeCartAddUuWindow.java]

实现思路
数据清洗转换
从 Kafka 中读取 JSON 字符串，转为 JSONObject。
设置 Watermark
提取 ts_ms 作为事件时间，使用单调递增 Watermark。
按用户分组
使用 KeyedStream 对每个用户进行分组。
状态管理去重
利用 ValueState<String> 记录用户上一次加购的日期；
如果当前加购日期不同于上次，则视为新用户，输出并更新状态；
设置状态 TTL 为一天，自动过期。
开窗聚合
使用 TumblingEventTimeWindow，每秒一个滚动窗口；
先聚合独立用户数（计数）；
再通过 AllWindowFunction 构建最终输出实体类 CartAddUuBean。
结果输出
转换为 JSON 字符串；
打印或写入 Doris。


[DwsTradeProvinceOrderWindow.java]

Kafka Source
↓
JSON 解析 + 过滤无效记录
↓
KeyBy 订单ID去重（幂等处理）
↓
Watermark 设置事件时间
↓
Map 转换为实体类对象
↓
KeyBy 省份ID 分组
↓
Tumbling Window 开窗聚合
↓
Async Join 维度表 dim_base_province
↓
Sink To Doris


1. 数据源读取与清洗
   从 Kafka 读取订单明细日志；
   忽略格式错误的数据，提高健壮性；
   设置消费者组、初始偏移量等参数。
2. KeyBy + ProcessFunction 实现幂等处理
   对 order_detail.id 做分组，保证同一订单只处理一次；
   利用状态管理实现“幂等”机制，旧数据取反发送，新数据正常发送，防止重复累加；
   设置 TTL 为 10 秒，避免状态无限增长。
3. 设置 Watermark 并提取事件时间
   使用 ts 字段作为事件时间；
   使用单调递增的 Watermark，适用于大多数电商场景；
   保障窗口计算的时间一致性。
4. Map 转换为实体类对象
   构建统一的业务对象，方便下游处理；
   使用 HashSet 存储订单 ID，用于去重计数。
5. KeyBy + TumblingWindow 开窗聚合
   按省份分组；
   使用 10 秒滚动窗口；
   使用 ReduceFunction 累加金额，使用 WindowFunction 设置窗口时间维度。
6. 异步关联维度表（Dim 表）
   使用异步 IO 提高性能；
   关联省份维度表获取省份名称；
   支持并发访问外部维度服务（如 HBase / MySQL）。
   为什么要对订单做幂等处理？
   因为 Kafka 消费可能重复消费，导致数据重复累加，需要幂等机制防止这种情况。
   为什么使用 Async I/O 关联维度表？
   避免阻塞主线程，提高查询效率，适用于并发访问外部系统。
   窗口函数使用 Reduce + WindowFunction 的组合有什么优势？
   Reduce 函数轻量高效，适合中间聚合；WindowFunction 用于补充窗口元信息。
   如果数据乱序严重怎么办？
   可以适当增加 Watermark 的延迟容忍时间，或者使用 Lateral Window。
   如何保障 Exactly-Once？
   开启 Checkpointing，并且 Sink 支持事务提交。
[DwsTradeSkuOrderWindow.java]
   1.主要用于对电商交易中的订单明细数据进行处理和分析。具体功能包括从 Kafka 中读取订单明细数据，对数据进行去重、时间窗口聚合，
   然后关联多个维度表（如商品信息、品牌信息、分类信息等）
   数据去重
   按照订单明细的 id 进行分组，使用 Flink 的状态管理机制对重复数据进行去重处理。
   时间窗口处理
   指定水印和事件时间字段，按照商品 skuId 进行分组，然后使用滚动时间窗口（10 秒）对数据进行聚合。
   维度关联
[DwsTrafficVcChArIsNewPageViewWindow.java]
   页面浏览是最基础也是最重要的用户行为之一。通过分析用户的访问数据，我们可以了解：
   不同设备版本的使用情况；
   各个地区的用户活跃度；
   渠道推广效果；
   新老用户的留存与参与度；
   这些维度可以帮助我们优化产品设计、调整运营策略、提升用户体验。
   类别	内容
   输入	Kafka 中的 DWD 层页面日志（JSON 格式），包含 mid、ts、common、page 等字段
   输出	每 10 秒按 vc（版本）、ch（渠道）、ar（地区）、is_new（是否新用户） 维度聚合以下指标：<br>• 页面浏览数（PV）<br>• 独立访客数（UV）<br>• 跳出次数（SV）<br>• 停留总时长（durSum）
   存储	将结果写入 Apache Doris，便于后续 BI 查询和可视化

Kafka Source
↓
解析 JSON 字符串 → JSONObject
↓
KeyBy(mid) → 判断是否为 UV & SV
↓
设置 Watermark + EventTime
↓
KeyBy(vc, ch, ar, is_new)
↓
开窗（Tumbling 10s）+ Reduce 聚合
↓
Sink To Doris


为什么只统计页面浏览？
页面浏览是用户行为的基础，是衡量活跃度和体验的核心指标。
如何判断用户是新用户？
根据 common.is_new 字段，由埋点系统打标决定。
为什么使用 mid 作为唯一标识？
mid 是设备级别标识，适合用来统计 UV。


[DwsUserUserLoginWindow.java]
用户登录是一个重要的行为事件，通过分析用户的登录频率与间隔，可以判断：
活跃用户数量（UU）；
用户是否流失后又重新回归（回流用户）；
Kafka 中的 DWD 层页面日志（JSON 格式），包含 common.uid、page.last_page_id、ts 等字段
输出	每 10 秒统计一次：独立用户数（UU）、回流用户数（Back）
存储	将结果写入 Apache Doris，便于后续 BI 查询和可视化
Kafka Source
↓
解析 JSON 字符串 → JSONObject
↓
过滤出登录行为（last_page_id = 'login' 或 null）
↓
设置 Watermark + EventTime
↓
KeyBy(uid) → 判断是否为新用户/回流用户
↓
开窗（Tumbling 10s）+ Reduce 聚合
↓
Sink To Doris

为什么只统计登录行为？
登录是用户身份确认的关键节点，是活跃度的核心指标。
如何判断用户是回流用户？
如果用户上次登录距离本次登录超过 8 天，则认为该用户回流。
为什么使用 windowAll 而不是 keyedWindow？
我们需要的是全局的统计值，而不是每个用户维度的数据。
如果系统重启，状态会不会丢失？
不会，我们启用了 Checkpoint，并设置了状态后端为 RocksDB 或 FsStateBackend。
如何验证结果准确性？
可以通过日志打印中间数据、对比离线 Hive 统计等方式验证。